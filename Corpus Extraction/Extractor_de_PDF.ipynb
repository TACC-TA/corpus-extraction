{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xEaAuuFlrG8",
        "outputId": "c407bee2-7cf5-47c3-d018-c85086aa3f65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (42.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=4d11813072d5c64c6bb760fbe21f6350cc8a834790c9edaafde0ef5e91d89850\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect, pdfminer.six\n",
            "Successfully installed langdetect-1.0.9 pdfminer.six-20231228\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer.six langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "import pandas as pd\n",
        "from langdetect import detect, DetectorFactory\n",
        "import re\n",
        "\n",
        "# Establece un seed en langdetect para obtener resultados consistentes\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# Nombre de tu archivo PDF\n",
        "nombre_del_archivo = 'huay_shlp7.pdf'\n",
        "\n",
        "# Función para detectar el idioma del texto\n",
        "def detectar_idioma(texto):\n",
        "    try:\n",
        "        return detect(texto)\n",
        "    except:\n",
        "        return \"no detectado\"\n",
        "\n",
        "# Función para extraer y filtrar el texto por idioma\n",
        "def extraer_y_filtrar_texto_por_idioma(nombre_del_archivo):\n",
        "    texto = extract_text(nombre_del_archivo)\n",
        "    # Usar expresiones regulares para encontrar bloques de palabras\n",
        "    bloques = re.split(r'\\.\\s+', texto)  # Divide el texto en bloques por puntos\n",
        "    # Filtrar bloques, manteniendo aquellos que no son detectados como español\n",
        "    bloques_quechua = [bloque for bloque in bloques if detectar_idioma(bloque) != 'es']\n",
        "    return ' '.join(bloques_quechua)\n",
        "\n",
        "# Extraer y filtrar el texto\n",
        "texto_filtrado = extraer_y_filtrar_texto_por_idioma(nombre_del_archivo)\n",
        "\n",
        "# Crear un DataFrame con pandas\n",
        "df = pd.DataFrame([texto_filtrado], columns=['Texto'])\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "df.to_csv('huay_shlp7.csv', index=False)\n",
        "\n",
        "# Mostrar un mensaje de éxito\n",
        "print(\"El texto ha sido extraído, filtrado por idioma y guardado en 'texto_filtrado.csv'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bo3vC0wlti2",
        "outputId": "a416e0e7-4504-41a5-9330-53938426d6dd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El texto ha sido extraído, filtrado por idioma y guardado en 'texto_filtrado.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from pdfminer.high_level import extract_text\n",
        "#import pandas as pd\n",
        "#from langdetect import detect, DetectorFactory\n",
        "#import re\n",
        "\n",
        "# Establece un seed en langdetect para obtener resultados consistentes\n",
        "#DetectorFactory.seed = 0\n",
        "\n",
        "# Nombre de tu archivo PDF\n",
        "#nombre_del_archivo = 'tu_archivo.pdf'\n",
        "\n",
        "# Función para detectar el idioma del texto\n",
        "#def detectar_idioma(texto):\n",
        "#    try:\n",
        "#        return detect(texto)\n",
        "#    except:\n",
        "#        return \"no detectado\"\n",
        "\n",
        "# Función para extraer y filtrar el texto por idioma\n",
        "#def extraer_y_filtrar_texto_por_idioma(nombre_del_archivo):\n",
        "#    texto = extract_text(nombre_del_archivo)\n",
        "    # Usar expresiones regulares para encontrar bloques de palabras\n",
        "#    bloques = re.split(r'\\.\\s+', texto)  # Divide el texto en bloques por puntos\n",
        "    # Filtrar bloques, manteniendo aquellos que no son detectados como español\n",
        "#    bloques_quechua = [bloque for bloque in bloques if detectar_idioma(bloque) != 'es']\n",
        "#    return bloques_quechua\n",
        "\n",
        "# Extraer y filtrar el texto\n",
        "#texto_filtrado = extraer_y_filtrar_texto_por_idioma(nombre_del_archivo)\n",
        "\n",
        "# Crear un DataFrame con pandas donde cada párrafo es una fila\n",
        "#df = pd.DataFrame(texto_filtrado, columns=['Texto'])\n",
        "\n",
        "# Guardar el DataFrame en un archivo CSV\n",
        "#df.to_csv('texto_filtrado.csv', index=False)\n",
        "\n",
        "# Mostrar un mensaje de éxito\n",
        "#print(\"El texto ha sido extraído, filtrado por idioma y guardado en 'texto_filtrado.csv'\")\n"
      ],
      "metadata": {
        "id": "TjM4mJx-5sQS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}